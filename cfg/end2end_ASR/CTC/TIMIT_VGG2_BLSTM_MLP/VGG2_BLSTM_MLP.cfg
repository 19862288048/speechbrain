[global]
    feature_computations=$feature_computations
[/global]

[functions]

  [compute_features]
      class_name=speechbrain.core.execute_computations
      cfg_file=$feature_computations
      out_var=FBANKs
      stop_at=FBANKs
      torch_no_grad=True
  [/compute_features]

	[mean_var_norm]
	    class_name=speechbrain.processing.features.mean_var_norm
	    norm_type=global
      torch_no_grad=True
	[/mean_var_norm]

  [convnet]
      class_name=speechbrain.core.execute_computations
      cfg_file=$CNN_computations
      out_var=out
      replicate=2
      replicate_with=conv2d,kernel_size=3:3*2 conv2d,out_channels=128,256 conv2d2,kernel_size=3:3*2 conv2d2,out_channels=128,256
  [/convnet]

  [MLP]
      class_name=speechbrain.core.execute_computations
      cfg_file=$MLP_computations
      out_var=out
      replicate=2
  [/MLP]

	[RNN]
		  class_name=speechbrain.nnet.architectures.RNN_basic
      rnn_type=lstm
      n_neurons=512
      nonlinearity=tanh
      num_layers=4
      dropout=0.15
      bidirectional=True
	[/RNN]

	[lin]
		  class_name=speechbrain.nnet.architectures.linear
		  n_neurons=40 # 39 phonemes + blank (at the end)
		  bias=False
	[/lin]


	[log_softmax]
		  class_name=speechbrain.nnet.architectures.activation
		  act_type=log_softmax
	[/log_softmax]

  [compute_cost]
	    class_name=speechbrain.nnet.losses.compute_cost
	    cost_type=ctc
  [/compute_cost]

  [compute_cost_wer]
	    class_name=speechbrain.nnet.losses.compute_cost
	    cost_type=ctc,wer
  [/compute_cost_wer]

  [optimizer]
	    class_name=speechbrain.nnet.optimizers.optimize
	    recovery=True
	    optimizer_type=adadelta
      rho=0.95
	    learning_rate=$lr
  [/optimizer]

  [print_predictions]
	    class_name=speechbrain.data_io.data_io.print_predictions
      ind2lab=phn
      ctc_out = True
      out_file=$output_folder/predictions.csv
  [/print_predictions]

[/functions]


[computations]

    id,wav,wav_len, phn,phn_len,batch_id,loop_id,mode,*_=get_input_var()

    feats=compute_features(wav)
    feats = mean_var_norm(feats,wav_len)

    out=convnet(feats)
    out=RNN(out)
    out=MLP(out)
    out=lin(out)

    pout=log_softmax(out)

    if mode == 'train':
        loss=compute_cost(pout,phn,[wav_len,phn_len])
        loss.backward()
        optimizer(convnet,RNN,MLP,lin)

    if mode == 'valid':
    	loss, wer=compute_cost_wer(pout,phn,[wav_len,phn_len])

    if mode == 'test':
        print_predictions(id,pout,wav_len)

[/computations]
