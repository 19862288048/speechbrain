[global]
    feature_computations=$feature_computations
[/global]

[functions]    

	[compute_features]
	    class_name=speechbrain.core.execute_computations 
            cfg_file=$feature_computations
 	    torch_no_grad=True
	[/compute_features]

	[mean_var_norm]
	    class_name=speechbrain.processing.features.mean_var_norm
	    norm_type=global
            torch_no_grad=True
	[/mean_var_norm]
	    
	[RNN]
		class_name=speechbrain.nnet.architectures.RNN_basic
                rnn_type=gru
                n_neurons=256
                nonlinearity=tanh
                num_layers=4
                dropout=0.15
                bidirectional=True
	[/RNN]

	   
	[lin]
		class_name=speechbrain.nnet.architectures.linear
		n_neurons=48 # 48 phonemes + blank (at the end)
		bias=False
	[/lin]


	[softmax]
		class_name=speechbrain.nnet.architectures.activation
		act_type=log_softmax
	[/softmax]

    	[compute_cost]
	    class_name=speechbrain.nnet.losses.compute_cost
	    cost_type=ctc
    	[/compute_cost]

    	[optimizer]
	    class_name=speechbrain.nnet.optimizers.optimize
	    recovery=True
	    optimizer_type=adam
	    learning_rate=$lr
    	[/optimizer]

    	[print_predictions]
	    class_name=speechbrain.data_io.data_io.print_predictions
            ind2lab=phn
            ctc_out = True
            output_file=$output_folder/predictions.txt
    	[/print_predictions]



[/functions]


[computations]
 
    id,wav,wav_len, phn,phn_len,batch_id,loop_id,mode,*_=get_input_var()
      
    feats=compute_features(wav)
    feats = mean_var_norm(feats,wav_len)

    out=RNN(feats)
    out=lin(out)
  
    pout=softmax(out)


    if mode=='valid' or mode=='train':
    	loss=compute_cost(pout,phn,[wav_len,phn_len])

    if mode == 'train':
        loss.backward()
        optimizer(RNN,lin)

    if mode=='test':
        print_predictions(id,pout,wav_len)

[/computations]



