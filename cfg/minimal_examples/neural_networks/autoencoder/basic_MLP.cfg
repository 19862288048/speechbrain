[global]
    feature_computations=$feature_computations
[/global]

[functions]    

	[compute_features]
	    class_name=lib.core.execute_computations 
            cfg_file=$feature_computations
 	    torch_no_grad=True
            cfg_change=functions,context_window,left_frames=1 functions,context_window,right_frames=1 
	[/compute_features]

	[mean_var_norm]
	    class_name=lib.processing.features.mean_var_norm
	    norm_type=global
            torch_no_grad=True
	[/mean_var_norm]
	    
        [linear1]
	    class_name=lib.nnet.architectures.linear
	    n_neurons=128
            bias = False
    	[/linear1]


        [activation]
	    class_name=lib.nnet.architectures.activation
            act_type=leaky_relu
    	[/activation]

	[linear2]
		class_name=lib.nnet.architectures.linear
		n_neurons=180
		bias=False
	[/linear2]


    	[compute_cost]
	    class_name=lib.nnet.losses.compute_cost
	    cost_type=mse
    	[/compute_cost]

    	[optimizer]
	    class_name=lib.nnet.optimizers.optimize
	    recovery=True
	    optimizer_type=adam
	    learning_rate=$lr
    	[/optimizer]



[/functions]


[computations]
 
    id,wav,wav_len, batch_id,loop_id,mode,*_=get_input_var()
      
    feats=compute_features(wav)
    feats = mean_var_norm(feats,wav_len)

    out=linear1(feats)
    out=activation(out)
    out=linear2(out)
  
    if mode=='valid' or mode=='train':
    	loss=compute_cost(out,feats,wav_len)
          
    if mode == 'train':
        loss.backward()
        optimizer(linear1,linear2)

[/computations]



