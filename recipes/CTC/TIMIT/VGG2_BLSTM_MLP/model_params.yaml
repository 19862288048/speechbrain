model: !torch.nn.Sequential
    - !recipes.features.Features
        feature_type: fbank
        requires_grad: False

    - !speechbrain.processing.features.mean_var_norm
        norm_type: global

    - !speechbrain.core.Replicate
        number_of_copies: 2
        module_list:
            - class_name: speechbrain.nnet.architectures.conv
              kwargs: {out_channels: 64, kernel_size: 3, padding: 0}
            - class_name: speechbrain.nnet.normalization.normalize
              kwargs: {norm_type: batchnorm}
            - class_name: speechbrain.nnet.architectures.activation
              kwargs: {act_type: leaky_relu}
            - class_name: speechbrain.nnet.architectures.conv
              kwargs: {out_channels: 64, kernel_size: 3, padding: 0}
            - class_name: speechbrain.nnet.normalization.normalize
              kwargs: {norm_type: batchnorm}
            - class_name: speechbrain.nnet.architectures.activation
              kwargs: {act_type: leaky_relu}
            - class_name: speechbrain.nnet.architectures.pooling
              kwargs: {kernel_size: 2, stride: 2, pool_type: max, pool_dim: 1}
            - class_name: speechbrain.nnet.architectures.dropout
              kwargs: {drop_rate: 0.15}
        override_list:
            - {0: {out_channels: 128}, 3: {out_channels: 128}}
            - {0: {out_channels: 256}, 3: {out_channels: 256}}

    - !speechbrain.nnet.architectures.RNN_basic
        rnn_type: lstm
        n_neurons: 512
        nonlinearity: tanh
        num_layers: 4
        dropout: 0.15
        bidirectional: True

    - !speechbrain.core.Replicate
        number_of_copies: 2
        module_list:
            - class_name: speechbrain.nnet.architectures.linear
              kwargs: {n_neurons: 512}
            - class_name: speechbrain.nnet.normalization.normalize
              kwargs: {norm_type: batchnorm}
            - class_name: speechbrain.nnet.architectures.activation
              kwargs: {act_type: leaky_relu}
            - class_name: speechbrain.nnet.architectures.dropout
              kwargs: {drop_rate: 0.15}

    - !speechbrain.nnet.architectures.linear
        n_neurons: 40 # 39 phonemes + blank (at the end)
        bias: False

    - !speechbrain.nnet.architectures.activation
        act_type: log_softmax
