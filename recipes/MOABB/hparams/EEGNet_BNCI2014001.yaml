###################################################
# Classification of imagined movements (right hand, left hand, feet, tongue) of BNCI2014001 MOABB dataset using EEGNet.
# EEGNet from https://doi.org/10.1088/1741-2552/aace8c.
# BNCI2014001 from https://doi.org/10.3389/fnins.2012.00055.
#
# Author
# ------
# Davide Borra, 2021
###################################################
seed: 1234
__set_torchseed: !apply:torch.manual_seed [!ref <seed>]

# DIRECTORIES
data_folder: !PLACEHOLDER  #'/path/to/BNCI2014001'. The dataset will be automatically downloaded in this folder
output_folder: !ref results/MOABB/EEGNet_BNCI2014001/<seed>

# DATASET HPARS
# Defining the MOABB dataset.
dataset: !new:moabb.datasets.BNCI2014001
subject_list: [1, 2, 3, 4, 5, 6, 7, 8, 9]  # subject list
events: ['left_hand', 'right_hand', 'feet', 'tongue']
n_classes: 4
sf: 128  # Target sampling rate (Hz)
# tmin, tmax respect to stimulus onset that define the interval attribute of the dataset class
# trial begins (0 s), cue (2 s, 1.25 s long); each trial is 6 s long
# dataset interval starts from 2
# -->tmin tmax are referred to this start value (e.g., tmin=0.5 corresponds to 2.5 s)
tmin: 0.5
tmax: 2.5
T: !ref <sf> * (<tmax> - <tmin>)
# fmin, fmax band-pass filtering
fmin: 4
fmax: 40
channels: ['Fz', 'FC3', 'FC1', 'FCz', 'FC2', 'FC4',
           'C5', 'C3', 'C1', 'Cz', 'C2', 'C4',
           'C6', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4',
           'P1', 'Pz', 'P2', 'POz']
C: 22
# Defining the MOABB paradigm and setting the pre-processing hyper-parameters for the specific dataset.
paradigm: !new:moabb.paradigms.MotorImagery
  events: !ref <events>
  n_classes: !ref <n_classes>
  fmin: !ref <fmin>
  fmax: !ref <fmax>
  tmin: !ref <tmin>
  tmax: !ref <tmax>
  channels: !ref <channels>
  resample: !ref <sf>

# TRAINING HPARS
num_parallel_processes: 6  # number of parallel processes to run
nfolds: 4 # number of cross-validation folds (only for within_session_it, cross_session_it)
number_of_epochs: 500 # number of training epochs
lr: 0.001

# Learning rate scheduling (cyclic learning rate is used here)
max_lr: !ref <lr> # Upper bound of the cycle (max value of the lr)
base_lr: 0.00000001 # Lower bound in the cycle (min value of the lr)
step_size: 100 # number of training iterations per half cycle (lower step_size => faster cycles)

# We here specify how to perfom test:
# - If test_with: 'last' we perform test with the latest model.
# - if test_with: 'best, we perform test with the best model (according to the metric specified in test_keys)
# The variable avg_models can be used to average the parameters of the last (or best) N saved models before testing.
# This can have a regularization effect. If avg_models: 1, the last (or best) model is used directly.
test_with: 'best' # 'last' or 'best'
test_keys: ["acc"] # Possible opts: "loss", "f1", "auc", "acc"
avg_models: 1 # checkpoints to average

loss: !name:speechbrain.nnet.losses.nll_loss
  label_smoothing: 0.0
  weight: torch.Tensor([1, 1, 1, 1])  # class weight to address class imbalance

optimizer: !name:torch.optim.Adam
  lr: !ref <lr>
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter  # epoch counter
  limit: !ref <number_of_epochs>
batch_size: 32
valid_ratio: 0.2

# Data iterators
within_session_it: !new:recipes.MOABB.dataio_iterators.WithinSession
  paradigm: !ref <paradigm>
  nfolds: !ref <nfolds>
  seed: !ref <seed>
leave_one_session_out_it: !new:recipes.MOABB.dataio_iterators.LeaveOneSessionOut
  paradigm: !ref <paradigm>
  seed: !ref <seed>
cross_session_it: !new:recipes.MOABB.dataio_iterators.CrossSession
  paradigm: !ref <paradigm>
  nfolds: !ref <nfolds>
  seed: !ref <seed>
leave_one_subject_out_it: !new:recipes.MOABB.dataio_iterators.LeaveOneSubjectOut
  paradigm: !ref <paradigm>
  seed: !ref <seed>
data_iterators: [!ref <within_session_it>,
                 !ref <leave_one_session_out_it>,
                 !ref <cross_session_it>,
                 !ref <leave_one_subject_out_it>,
]

# DATA AUGMENTATION
#channel_dropper: !new:speechbrain.processing.speech_augmentation.ChannelDrop
#    drop_rate: 0.1
      
#chunk_dropper: !new:speechbrain.processing.speech_augmentation.DropChunk
#    drop_length_low: 10
#    drop_length_high: 30
#    drop_count_low: 1
#    drop_count_high: 3
#    drop_start: 0
#    drop_prob: 1
#    noise_factor: 0.0

#augment: !new:speechbrain.processing.augmentation.Augmenter
#    parallel_augment: False
#    concat_original: True
#    min_augmentations: 1
#    max_augmentations: 2
#    repeat_augment: 1
#    shuffle_augmentations: False
#    chunk_dropper: !ref <chunk_dropper>
#    channel_dropper: !ref <channel_dropper>
    
    
# MODEL: EEGNET
input_shape: [null, !ref <T>, !ref <C>, null]
cnn_temporal_kernels: 8  # number of temporal filters in the temporal conv. layer
cnn_spatial_depth_multiplier: 2  # depth multiplier for the spatial depthwise conv. layer
cnn_spatial_max_norm: 1.  # kernel max-norm constaint of the spatial depthwise conv. layer
cnn_septemporal_depth_multiplier: 1  # depth multiplier for the separable temporal conv. layer
dense_max_norm: 0.25  # kernel max-norm constaint of the dense layer
dropout: 0.25  # dropout rate

model: !new:speechbrain.lobes.models.EEGNet.EEGNet
  input_shape: !ref <input_shape>
  sf: !ref <sf>
  cnn_temporal_kernels: !ref <cnn_temporal_kernels>
  cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>
  cnn_spatial_max_norm: !ref <cnn_spatial_max_norm>
  cnn_septemporal_depth_multiplier: !ref <cnn_septemporal_depth_multiplier>
  dense_n_neurons: !ref <n_classes>
  dense_max_norm: !ref <dense_max_norm>
  dropout: !ref <dropout>

lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler
    base_lr: !ref <base_lr>
    max_lr: !ref <max_lr>
    step_size: !ref <step_size>

