data_folder: /idiap/resource/database/LibriSpeech
save_folder: data
output_folder: exp-test

train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean"]
train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv
skip_prep: False

avoid_if_longer_than: 16.
avoid_if_shorter_than: 2.5
log_interval: 25
use_wandb: False
auto_mix_prec: True
max_grad_norm: 100.

number_of_epochs: 300
optimizer_step_limit: 400000
train_num_buckets: 70
train_num_workers: 4

sample_rate: 16000
lr: 0.002
warmup: 20000
seconds_per_batch: 160
encoder_layerdrop: 0.05

diversity_loss_weight: 0.2
mask_prob: 0.6
mask_length: 10

embedding_dim: 768
final_dim: 256
latentextractor_kernels: [11, 3, 3, 3, 3, 3, 3]
latentextractor_strides: [5, 2, 2, 2, 2, 2, 2]

optimizer: !name:torch.optim.AdamW
  lr: !ref <lr>
  weight_decay: 0.01
  eps: 0.000001

test_dataloader_options:
    batch_size: 8
    num_workers: 4

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

extractor: !new:speechbrain.lobes.models.wav2vec.W2VLatentExtractor
  kernel_sizes: !ref <latentextractor_kernels>
  strides: !ref <latentextractor_strides>
  out_channels: [128, 256, 256, 512, 512, 1024, 1024]

encoder: !new:speechbrain.lobes.models.wav2vec.W2VLatentEncoder
  embedding_dim: !ref <embedding_dim>
  num_layers: 12
  dropout: 0.1
  layerdrop_prob: !ref <encoder_layerdrop>

encoder_wrapper: !new:speechbrain.lobes.models.wav2vec.EncoderWrapper
  in_dim: 1024
  embedding_dim: !ref <embedding_dim>
  latent_encoder: !ref <encoder>
  dropout_encoder_input: 0.1

target_quantiser: !new:speechbrain.lobes.models.wav2vec.W2VTargetQuantiser
  in_dim: 1024
  out_dim: !ref <final_dim>

feat_proj: !new:torch.nn.Linear
  in_features: !ref <embedding_dim>
  out_features: !ref <final_dim>

modules:
  latent_extractor: !ref <extractor>
  latent_encoder: !ref <encoder_wrapper>
  feat_proj: !ref <feat_proj>
  target_quantiser: !ref <target_quantiser>

loss: !new:speechbrain.nnet.losses.ContrastiveLoss
  logit_temp: 0.1

lr_scheduler: !new:speechbrain.nnet.schedulers.WarmCoolDecayLRSchedule
    lr: !ref <lr>
    warmup: !ref <warmup>
    cooldown: 30000
    total_steps: !ref <optimizer_step_limit>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <output_folder>
    recoverables:
        latent_extractor: !ref <extractor>
        latent_encoder: !ref <encoder_wrapper>
        feat_proj: !ref <feat_proj>
        target_quantiser: !ref <target_quantiser>
        scheduler: !ref <lr_scheduler>
        counter: !ref <epoch_counter>
