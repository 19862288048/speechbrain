seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
data_folder: /idiap/resource/database/LibriSpeech
save_folder: data
output_folder: exp-test

train_splits: ["train-clean-100", "train-clean-360", "train-other-500"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]
train_csv: !ref <output_folder>/train.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv
skip_prep: False

avoid_if_longer_than: 16.
avoid_if_shorter_than: 2.5
log_interval: 25
use_wandb: False
auto_mix_prec: True
disable_halfprec: True

number_of_epochs: 9999
optimizer_step_limit: 1000000
train_num_buckets: 70
max_sample_dur: 16.0

sample_rate: 16000
lr: 0.005
warmup: 30000
seconds_per_batch: 160
encoder_layerdrop: 0.

diversity_loss_weight: 0.1
mask_prob: 0.65
mask_length: 10

embedding_dim: 1024
final_dim: 768
latentextractor_kernels: [11, 3, 3, 3, 3, 3, 3]
latentextractor_strides: [5, 2, 2, 2, 2, 2, 2]

optimizer: !name:torch.optim.AdamW
  lr: !ref <lr>
  weight_decay: 0.01
  eps: 0.000001

test_dataloader_options:
    batch_size: 8
    num_workers: 6

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

extractor: !new:speechbrain.lobes.models.wav2vec.W2VLatentExtractor
  kernel_sizes: !ref <latentextractor_kernels>
  strides: !ref <latentextractor_strides>

encoder: !new:speechbrain.lobes.models.wav2vec.W2VLatentEncoder
  embedding_dim: !ref <embedding_dim>
  num_layers: 24
  dropout: 0.
  d_ffn: 4096
  nhead: 16
  layerdrop_prob: !ref <encoder_layerdrop>

encoder_wrapper: !new:speechbrain.lobes.models.wav2vec.EncoderWrapper
  in_dim: 512
  embedding_dim: !ref <embedding_dim>
  latent_encoder: !ref <encoder>
  dropout_encoder_input: 0.

target_quantiser: !new:speechbrain.lobes.models.wav2vec.W2VTargetQuantiser
  in_dim: 512
  out_dim: !ref <final_dim>
  temperature_decay: [2.0, 0.25, 0.999995]

feat_proj: !new:torch.nn.Linear
  in_features: !ref <embedding_dim>
  out_features: !ref <final_dim>

modules:
  latent_extractor: !ref <extractor>
  latent_encoder: !ref <encoder_wrapper>
  feat_proj: !ref <feat_proj>
  target_quantiser: !ref <target_quantiser>

loss: !new:speechbrain.nnet.losses.ContrastiveLoss
  logit_temp: 0.1

lr_scheduler: !new:speechbrain.nnet.schedulers.WarmCoolDecayLRSchedule
    lr: !ref <lr>
    warmup: !ref <warmup>
    cooldown: 50000
    total_steps: !ref <optimizer_step_limit>

checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <output_folder>
    recoverables:
        latent_extractor: !ref <extractor>
        latent_encoder: !ref <encoder_wrapper>
        feat_proj: !ref <feat_proj>
        target_quantiser: !ref <target_quantiser>
        scheduler: !ref <lr_scheduler>
        counter: !ref <epoch_counter>
